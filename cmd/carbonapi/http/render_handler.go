package http

import (
	"github.com/ansel1/merry"
	"io/ioutil"
	"net/http"
	"strconv"
	"strings"
	"time"

	"github.com/go-graphite/carbonapi/carbonapipb"
	"github.com/go-graphite/carbonapi/cmd/carbonapi/config"
	"github.com/go-graphite/carbonapi/date"
	"github.com/go-graphite/carbonapi/expr"
	"github.com/go-graphite/carbonapi/expr/functions/cairo/png"
	"github.com/go-graphite/carbonapi/expr/types"
	"github.com/go-graphite/carbonapi/pkg/parser"
	// zipperErrors "github.com/go-graphite/carbonapi/zipper/types"
	utilctx "github.com/go-graphite/carbonapi/util/ctx"

	pb "github.com/go-graphite/protocol/carbonapi_v3_pb"
	"github.com/lomik/zapwriter"
	"github.com/satori/go.uuid"
	"go.uber.org/zap"
)

func cleanupParams(r *http.Request) {
	// make sure the cache key doesn't say noCache, because it will never hit
	r.Form.Del("noCache")

	// jsonp callback names are frequently autogenerated and hurt our cache
	r.Form.Del("jsonp")

	// Strip some cache-busters.  If you don't want to cache, use noCache=1
	r.Form.Del("_salt")
	r.Form.Del("_ts")
	r.Form.Del("_t") // Used by jquery.graphite.js
}

func setError(w http.ResponseWriter, accessLogDetails *carbonapipb.AccessLogDetails, msg string, status int) {
	http.Error(w, http.StatusText(status)+": "+msg, status)
	accessLogDetails.Reason = msg
	accessLogDetails.HTTPCode = int32(status)
}

func getCacheTimeout(logger *zap.Logger, r *http.Request) int32 {
	cacheTimeout := config.Config.Cache.DefaultTimeoutSec

	if tstr := r.FormValue("cacheTimeout"); tstr != "" {
		t, err := strconv.Atoi(tstr)
		if err != nil {
			logger.Error("failed to parse cacheTimeout",
				zap.String("cache_string", tstr),
				zap.Error(err),
			)
		} else {
			cacheTimeout = int32(t)
		}
	}

	return cacheTimeout
}

func renderHandler(w http.ResponseWriter, r *http.Request) {
	t0 := time.Now()
	uuid := uuid.NewV4()

	// TODO: Migrate to context.WithTimeout
	// ctx, _ := context.WithTimeout(context.TODO(), config.Config.ZipperTimeout)
	ctx := utilctx.SetUUID(r.Context(), uuid.String())
	username, _, _ := r.BasicAuth()
	requestHeaders := utilctx.GetLogHeaders(ctx)

	logger := zapwriter.Logger("render").With(
		zap.String("carbonapi_uuid", uuid.String()),
		zap.String("username", username),
		zap.Any("request_headers", requestHeaders),
	)

	srcIP, srcPort := splitRemoteAddr(r.RemoteAddr)

	accessLogger := zapwriter.Logger("access")
	var accessLogDetails = &carbonapipb.AccessLogDetails{
		Handler:        "render",
		Username:       username,
		CarbonapiUUID:  uuid.String(),
		URL:            r.URL.RequestURI(),
		PeerIP:         srcIP,
		PeerPort:       srcPort,
		Host:           r.Host,
		Referer:        r.Referer(),
		URI:            r.RequestURI,
		RequestHeaders: requestHeaders,
	}

	logAsError := false
	defer func() {
		deferredAccessLogging(accessLogger, accessLogDetails, t0, logAsError)
	}()

	size := 0
	ApiMetrics.Requests.Add(1)

	err := r.ParseForm()
	if err != nil {
		setError(w, accessLogDetails, err.Error(), http.StatusBadRequest)
		logAsError = true
		return
	}

	targets := r.Form["target"]
	from := r.FormValue("from")
	until := r.FormValue("until")
	template := r.FormValue("template")
	useCache := !parser.TruthyBool(r.FormValue("noCache"))
	noNullPoints := parser.TruthyBool(r.FormValue("noNullPoints"))
	// status will be checked later after we'll setup everything else
	format, ok, formatRaw := getFormat(r, pngFormat)

	var jsonp string

	if format == jsonFormat {
		// TODO(dgryski): check jsonp only has valid characters
		jsonp = r.FormValue("jsonp")
	}

	timestampFormat := strings.ToLower(r.FormValue("timestampFormat"))
	if timestampFormat == "" {
		timestampFormat = "s"
	}

	timestampMultiplier := int64(1)
	switch timestampFormat {
	case "s":
		timestampMultiplier = 1
	case "ms", "millisecond", "milliseconds":
		timestampMultiplier = 1000
	case "us", "microsecond", "microseconds":
		timestampMultiplier = 1000000
	case "ns", "nanosecond", "nanoseconds":
		timestampMultiplier = 1000000000
	default:
		setError(w, accessLogDetails, "unsupported timestamp format, supported: 's', 'ms', 'us', 'ns'", http.StatusBadRequest)
		logAsError = true
		return
	}

	cacheTimeout := getCacheTimeout(logger, r)

	cleanupParams(r)

	cacheKey := r.Form.Encode()

	// normalize from and until values
	qtz := r.FormValue("tz")
	from32 := date.DateParamToEpoch(from, qtz, timeNow().Add(-24*time.Hour).Unix(), config.Config.DefaultTimeZone)
	until32 := date.DateParamToEpoch(until, qtz, timeNow().Unix(), config.Config.DefaultTimeZone)

	accessLogDetails.UseCache = useCache
	accessLogDetails.FromRaw = from
	accessLogDetails.From = from32
	accessLogDetails.UntilRaw = until
	accessLogDetails.Until = until32
	accessLogDetails.Tz = qtz
	accessLogDetails.CacheTimeout = cacheTimeout
	accessLogDetails.Format = formatRaw
	accessLogDetails.Targets = targets

	if !ok || !format.ValidRenderFormat() {
		setError(w, accessLogDetails, "unsupported format specified: "+formatRaw, http.StatusBadRequest)
		logAsError = true
		return
	}

	if format == protoV3Format {
		body, err := ioutil.ReadAll(r.Body)
		if err != nil {
			accessLogDetails.HTTPCode = http.StatusBadRequest
			accessLogDetails.Reason = "failed to parse message body: " + err.Error()
			http.Error(w, "bad request (failed to parse format): "+err.Error(), http.StatusBadRequest)
			return
		}

		var pv3Request pb.MultiFetchRequest
		err = pv3Request.Unmarshal(body)

		if err != nil {
			accessLogDetails.HTTPCode = http.StatusBadRequest
			accessLogDetails.Reason = "failed to parse message body: " + err.Error()
			http.Error(w, "bad request (failed to parse format): "+err.Error(), http.StatusBadRequest)
			return
		}

		from32 = pv3Request.Metrics[0].StartTime
		until32 = pv3Request.Metrics[0].StopTime
		targets = make([]string, len(pv3Request.Metrics))
		for i, r := range pv3Request.Metrics {
			targets[i] = r.PathExpression
		}
	}

	if useCache {
		tc := time.Now()
		response, err := config.Config.QueryCache.Get(cacheKey)
		td := time.Since(tc).Nanoseconds()
		ApiMetrics.RenderCacheOverheadNS.Add(td)

		accessLogDetails.CarbonzipperResponseSizeBytes = 0
		accessLogDetails.CarbonapiResponseSizeBytes = int64(len(response))

		if err == nil {
			ApiMetrics.RequestCacheHits.Add(1)
			writeResponse(w, response, format, jsonp)
			accessLogDetails.FromCache = true
			return
		}
		ApiMetrics.RequestCacheMisses.Add(1)
	}

	if from32 == until32 {
		setError(w, accessLogDetails, "Invalid or empty time range", http.StatusBadRequest)
		logAsError = true
		return
	}

	var results []*types.MetricData
	errors := make(map[string]merry.Error)
	metricMap := make(map[parser.MetricRequest][]*types.MetricData)

	var metrics []string
	// Iterate over all targets, check if we need to fetch them
	for _, target := range targets {
		exp, e, err := parser.ParseExpr(target)

		// if expression cannot be parsed return error
		if err != nil || e != "" {
			msg := buildParseErrorString(target, e, err)
			setError(w, accessLogDetails, msg, http.StatusBadRequest)
			logAsError = true
			return
		}

		// Splitting requests into batches is now done by carbonzipper
		pathExprTimeMap := make(map[string]requestInterval)
		var req pb.MultiFetchRequest
		for _, m := range exp.Metrics() {
			metrics = append(metrics, m.Metric)
			mFetch := m
			mFetch.From += from32
			mFetch.Until += until32

			if _, ok := metricMap[mFetch]; ok {
				// we already have this metric in fetch queue
				continue
			}
			metricMap[mFetch] = make([]*types.MetricData, 0, 1)

			req.Metrics = append(req.Metrics, pb.FetchRequest{
				Name:           m.Metric,
				PathExpression: m.Metric,
				StartTime:      mFetch.From,
				StopTime:       mFetch.Until,
			})
			pathExprTimeMap[m.Metric] = requestInterval{from: mFetch.From, until: mFetch.Until}
		}

		// Do we need to fetch anything?
		if len(req.Metrics) > 0 {
			ApiMetrics.RenderRequests.Add(1)
			config.Config.Limiter.Enter()

			r, stats, err := config.Config.ZipperInstance.Render(ctx, req)
			if stats != nil {
				accessLogDetails.ZipperRequests += stats.ZipperRequests
				accessLogDetails.TotalMetricsCount += stats.TotalMetricsCount
			}
			if err != nil {
				errors[target] = err
			}

			config.Config.Limiter.Leave()
			for _, m := range r {
				var from int64
				var until int64
				if m.RequestStartTime != 0 && m.RequestStopTime != 0 {
					from = m.RequestStartTime
					until = m.RequestStopTime
				} else {
					from = pathExprTimeMap[m.PathExpression].from
					until = pathExprTimeMap[m.PathExpression].until
				}

				mFetch := parser.MetricRequest{
					Metric: m.PathExpression,
					From:   from,
					Until:  until,
				}

				d := metricMap[mFetch]
				metricMap[mFetch] = append(d, m)

			}
			for i := range r {
				size += r[i].Size()
			}

			for mFetch := range metricMap {
				expr.SortMetrics(metricMap[mFetch], mFetch)
			}
		}
		// Remove metrics for which fetch failed
		filteredMetricMap := make(map[parser.MetricRequest][]*types.MetricData)
		for k := range metricMap {
			if len(metricMap[k]) != 0 {
				filteredMetricMap[k] = metricMap[k]
			}
		}
		metricMap = filteredMetricMap
		accessLogDetails.Metrics = metrics
		if len(metricMap) > 0 {

			var rewritten bool
			var newTargets []string

			rewritten, newTargets, err = expr.RewriteExpr(exp, from32, until32, metricMap)
			if err != nil && !merry.Is(err, parser.ErrSeriesDoesNotExist) {
				errors[target] = merry.Wrap(err)
				accessLogDetails.Reason = err.Error()
				logAsError = true
				return
			}

			if rewritten {
				targets = append(targets, newTargets...)
			} else {
				func() {
					defer func() {
						if r := recover(); r != nil {
							logger.Error("panic during eval:",
								zap.String("cache_key", cacheKey),
								zap.Any("reason", r),
								zap.Stack("stack"),
							)
						}
					}()
					expressions, err := expr.EvalExpr(exp, from32, until32, metricMap)
					if err != nil && !merry.Is(err, parser.ErrSeriesDoesNotExist) {
						errors[target] = merry.Wrap(err)
						accessLogDetails.Reason = err.Error()
						logAsError = true
						return
					}
					results = append(results, expressions...)
				}()
			}
		}
	}

	var body []byte

	returnCode := http.StatusOK
	if len(results) == 0 {
		// Obtain error code from the errors
		// In case we have only "Not Found" errors, result should be 404
		// Otherwise it should be 500
		for _, err := range errors {
			if returnCode < 500 {
				returnCode = merry.HTTPCode(err)
			}
		}
		logger.Debug("empty response or no response")
		// Allow override status code for 404-not-found replies.
		if returnCode == 404 {
			returnCode = config.Config.NotFoundStatusCode

		}
		if returnCode < 300 {
			results = append(results, &types.MetricData{})
		} else {
			setError(w, accessLogDetails, "empty or no response", returnCode)
			logAsError = true
			return
		}
	}

	switch format {
	case jsonFormat:
		if maxDataPoints, _ := strconv.Atoi(r.FormValue("maxDataPoints")); maxDataPoints != 0 {
			types.ConsolidateJSON(maxDataPoints, results)
		}

		body = types.MarshalJSON(results, timestampMultiplier, noNullPoints)
	case protoV2Format:
		body, err = types.MarshalProtobufV2(results)
		if err != nil {
			setError(w, accessLogDetails, err.Error(), http.StatusInternalServerError)
			logAsError = true
			return
		}
	case protoV3Format:
		body, err = types.MarshalProtobufV3(results)
		if err != nil {
			setError(w, accessLogDetails, err.Error(), http.StatusInternalServerError)
			logAsError = true
			return
		}
	case rawFormat:
		body = types.MarshalRaw(results)
	case csvFormat:
		body = types.MarshalCSV(results)
	case pickleFormat:
		body = types.MarshalPickle(results)
	case pngFormat:
		body = png.MarshalPNGRequest(r, results, template)
	case svgFormat:
		body = png.MarshalSVGRequest(r, results, template)
	}

	w.WriteHeader(returnCode)
	writeResponse(w, body, format, jsonp)

	if len(results) != 0 {
		tc := time.Now()
		config.Config.QueryCache.Set(cacheKey, body, cacheTimeout)
		td := time.Since(tc).Nanoseconds()
		ApiMetrics.RenderCacheOverheadNS.Add(td)
	}

	gotErrors := len(errors) > 0
	accessLogDetails.HaveNonFatalErrors = gotErrors
}
